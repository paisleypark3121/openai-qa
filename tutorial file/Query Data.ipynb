{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask the Expert: How to Chat with a Podcast\n",
    "Welcome to this interactive tutorial where we'll explore how to \"talk\" to our data using the power of Large Language Models (LLMs) like OpenAI's GPT-3.5-turbo and Vector Databases. \n",
    "\n",
    "This tutorial will guide you through an exciting process of transforming unstructured data (a PDF document in our case) into an interactive and smart knowledge base that can answer your questions!\n",
    "\n",
    "By following this tutorial, you will:\n",
    "\n",
    "- Learn how to load data from a PDF file and split it into smaller, manageable chunks.\n",
    "- Understand the concept of text embeddings and how we can utilize them to store our data in a Vector Database.\n",
    "- Discover how to ask questions and retrieve the most relevant information from your database.\n",
    "- Use a Large Language Model to generate answers based on the context we provide.\n",
    "\n",
    "You will experience the benefit of harnessing the power of language models and vector databases in extracting and utilizing information from large amounts of text data. \n",
    "\n",
    "The approach used in this tutorial can be applied to a wide range of tasks, from creating a smart Q&A system to building a personal digital assistant or even designing a conversational AI.\n",
    "\n",
    "<img src=\"images/qa_flow.png\" alt=\"Image Alt Text\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing packages\n",
    "Before we start, you need to have a few Python libraries installed. You can install these libraries by running the following command in your terminal:\n",
    "```bash\n",
    "pip install openai langchain ipykernel python-dotenv chromadb pypdf tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading OpenAI API Key\n",
    "\n",
    "We'll need it for Embeddings and the GPT-3.5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading PDFs\n",
    "In this step, we're using the PyPDFLoader class from the LangChain library to load our PDF file into standardized Document format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# create an instance of PyPDFLoader with the target PDF file\n",
    "loader = PyPDFLoader(\"transcripts/PT693-Transcript.pdf\")\n",
    "\n",
    "# load the PDF file into a variable named 'docs'\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, langchain.schema.document.Document)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs), type(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Splitting Pages\n",
    "Next, we're using a `RecursiveCharacterTextSplitter` to break down the content of the PDF into smaller chunks.\n",
    "\n",
    "We define a chunk size and overlap to decide how large each slice should be and how much they should overlap with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Constants: Define constants used in the code.\n",
    "CHUNK_SIZE = 1200\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "# Create an instance of RecursiveCharacterTextSplitter with the desired chunk size and overlap\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP, \n",
    "    length_function=len  # function used to measure chunk size\n",
    ")\n",
    "\n",
    "# split documents into smaller chunks\n",
    "splits = r_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "1148\n",
      "960\n",
      "1157\n",
      "1014\n",
      "1193\n",
      "1002\n",
      "1161\n",
      "1091\n",
      "1152\n"
     ]
    }
   ],
   "source": [
    "for doc in splits[:10]:\n",
    "    print(len(doc.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Creating Embeddings for the Splits\n",
    "\n",
    "**Computers don't understand words. They only understand numbers.**\n",
    "\n",
    "Embeddings are a way of converting text into a numerical form that a machine can understand.\n",
    "\n",
    "Imagine trying to describe a movie scene to a friend - you would use words to describe what's happening, the mood, the characters, etc. In a similar way, embeddings capture the essence of the text, but in a format that the machine can work with, like a numerical vector.\n",
    "\n",
    "\n",
    "\n",
    "To create the embeddings, we're using OpenAIEmbeddings from LangChain. \n",
    "\n",
    "<img src=\"images/Embeddings.png\" alt=\"Image Alt Text\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Create an instance of OpenAIEmbeddings for embedding the chunks\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Search and Cosine Similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"I love dogs\"\n",
    "sent2 = \"I love cats\"\n",
    "sent3 = \"Yesterday I played basketball\"\n",
    "sent4 = \"Yesterday I played football\"\n",
    "sent5 = \"Leonardo Di Caprio is an underrated actor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(sent1)\n",
    "embedding2 = embedding.embed_query(sent2)\n",
    "embedding3 = embedding.embed_query(sent3)\n",
    "embedding4 = embedding.embed_query(sent4)\n",
    "embedding5 = embedding.embed_query(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9113747123840847\n",
      "0.9424351991605048\n",
      "---\n",
      "0.7609000588250058\n",
      "0.7659245559775505\n",
      "0.7612783173640613\n",
      "0.745260925638238\n",
      "0.7202260133268322\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Compute the dot product of vec1 and vec2\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    \n",
    "    # Compute the L2 norms (or magnitudes) of vec1 and vec2\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    # Compute the cosine similarity\n",
    "    cos_sim = dot_product / (norm_vec1 * norm_vec2)\n",
    "    \n",
    "    return cos_sim\n",
    "\n",
    "# Assuming vec1 and vec2 are your embeddings\n",
    "vec1 = np.array(embedding1)\n",
    "vec2 = np.array(embedding2)\n",
    "vec3 = np.array(embedding3)\n",
    "vec4 = np.array(embedding4)\n",
    "vec5 = np.array(embedding5)\n",
    "\n",
    "# More similar\n",
    "print(cosine_similarity(vec1, vec2))\n",
    "print(cosine_similarity(vec3, vec4))\n",
    "\n",
    "print(\"---\")\n",
    "# Less similar\n",
    "print(cosine_similarity(vec1, vec3))\n",
    "print(cosine_similarity(vec2, vec3))\n",
    "print(cosine_similarity(vec2, vec4))\n",
    "print(cosine_similarity(vec2, vec5))\n",
    "print(cosine_similarity(vec4, vec5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Storing them into a Vector Database\n",
    "\n",
    "A vector database is like a library for these numerical vectors. We store these vectors in a structured manner so we can search and retrieve them efficiently later on.\n",
    "\n",
    "Once we have the embeddings (the 'numerical' form of our text), we're storing them in a vector database using the Chroma class from LangChain.\n",
    "\n",
    "<img src=\"images/VectorDatabaseCreate.png\" alt=\"Image Alt Text\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Define directory to persist the embeddings\n",
    "persist_directory = 'chroma/sds/'\n",
    "\n",
    "# Create an instance of Chroma with the documents, embeddings, and the persist directory\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Retrieving Relevant Documents\n",
    "After storing the embeddings in the vector database (Chroma), we want to retrieve the most relevant ones based on our question. \n",
    "\n",
    "This is similar to asking a librarian for the most relevant books based on the topic you're interested in.\n",
    "\n",
    "This is what the `similarity_search` method does. It takes our question and returns the most related documents from our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Chroma with the persist directory and embeddings\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the created vector database to find the most similar documents to a given question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give me all books from the episode\"\n",
    "\n",
    "# Retrieve similar documents to a given question using the vector database\n",
    "docs = vectordb.similarity_search(question, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Harpreet Sahota:  01:15:00  Yeah, yeah, yeah. The Manga Guide to Calculus and the \\nManga Guide to Linear Algebra. Super good.  \\nJon Krohn:  01:15:06  Awesome. So, near the end of every episode I ask people \\nfor book recommendations, but you have just given us a \\nton. So, I think we've covered that question, unless you \\nhave any other books you'd like to add.  \\nHarpreet Sahota:  01:15:17  You know, I used to, I used to, I have, I've traded, when I \\nwas recording the Artist of Data Science podcast, I read a \\nlot of books lik e, cause I had so many authors on and \\nsince I kind of put the podcast on hold for now, I spent \\nmost of my time reading research papers in the morning \\nwhenever I have free mornings. I have not read a book in \\nlike six months, sadly. But the one that I have c urrently \\njust gone back to rereading is Deep Work by Cal Newport. \\nI think that's a good book. Important book for people who \\nare in roles like ours that are knowledge -intensive and \\nrequire a lot of thinking.  \\nJon Krohn:  01:15:55  Yeah. So, important to be abl e to carve out a little bit of \\ntime every day to be able to work deeply. It's absolutely\", metadata={'page': 39, 'source': 'transcripts/PT693-Transcript.pdf'}),\n",
       " Document(page_content=\"podcast directly by asking Professor Wiggins your burning \\nquestions on stage.  \\n 01:19:25  All right, thanks to my colleagues at Nebula for \\nsupporting me while I create content like this \\nSuperDataSc ience episode for you. And thanks of course \\nto Ivana, Mario, Natalie, Serg, Sylvia, Zara, and Kirill on \\nthe SuperDataScience team for producing another eye -\\nopening episode for us today. For enabling that super \\nteam to create this free podcast for you, we'r e of course, \\ndeeply grateful to our sponsors. Please consider \\nsupporting the show by checking out our sponsors' links, \\nwhich you can find in the show notes. Finally, thanks of \\ncourse to you for listening. I'm so grateful to have you \\ntuning in and I hope I can continue to make episodes you \\nlove for years and years to come. Well, until next time, my \\nfriend, keep on rocking it out there and I'm looking\", metadata={'page': 41, 'source': 'transcripts/PT693-Transcript.pdf'}),\n",
       " Document(page_content='forward to enjoying another round of the \\nSuperDataScience podcast with you very soon.', metadata={'page': 42, 'source': 'transcripts/PT693-Transcript.pdf'}),\n",
       " Document(page_content=\"time investment. So, like he wrote this book, the Deep \\nLearning Illustrated Guide, huge, huge, massive book, \\nright. No math - \\nJon Krohn:  01:08:48  Deep Learning: A Visual Approach.  \\nHarpreet Sahota:  01:08:49  Oh yes. Deep Learning: A Visual Approach. Yes, Deep \\nLearning Illustrated is another book I'm about to talk \\nabout. Deep Learning Illustrated Approach. Great book. \\nAnd then once you do that start le arning some PyTorch, \\nright. Just, you need to move away from SciKit -Learn. \\nGoing from SciKit -Learn to PyTorch is a bit of a mental \\nshift. But you know, Daniel Bourke, I'm not sure if you've \\ninterviewed him on your podcast or not, he's awesome. \\nHe's based o ut of Australia. Highly recommend him. \\n@mrdburke on Twitter. But he's got this Zero to Mastery \\nPyTorch course. Go through that because you're gonna \\nget a bit of intuition about what's happening under the \\nhood. Then you're getting your fingers on the keyboa rd, \\nyou're getting your hands dirty, you're coding, right? This\", metadata={'page': 35, 'source': 'transcripts/PT693-Transcript.pdf'}),\n",
       " Document(page_content=\"curriculum. But yeah, whether you are yeah, wanting to \\nget just get started on the underly ing calculus to \\nunderstand backprop or you want to jump to later videos \\nand get deep into the weeds on how backprop works \\nusing calculus principles and do it in a hands -on Python -\\nbased way. Yeah, I think I I it's my own resource, but I \\nthink it's good.  \\nHarpreet Sahota:  01:14:32  I've linked to, linked to that resource many times, like it's \\na great YouTube course. And another kind of interesting \\nresource I like is there's like this a series of manga books \\nthat touch on a wide range of topics. I've got the ent ire \\nset, but there's a book there on calculus and on linear \\nalgebra and they're like, you know, proper like comic \\nbooks, but it teaches you calculus and algebra.  \\nJon Krohn:  01:14:58  Oh, manga. Oh really?\", metadata={'page': 38, 'source': 'transcripts/PT693-Transcript.pdf'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7724149422983935\n",
      "0.7522458412266196\n",
      "0.7437202518308199\n",
      "0.7277780673958708\n",
      "0.7273885797063984\n"
     ]
    }
   ],
   "source": [
    "q_emb = embedding.embed_query(question)\n",
    "q_vec = np.array(q_emb)\n",
    "\n",
    "for d in docs:\n",
    "    emb = embedding.embed_query(d.page_content)\n",
    "    vec = np.array(emb)\n",
    "    cosine = cosine_similarity(q_vec, vec)\n",
    "    print(cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Generating the Answer\n",
    "In this step, we use our Large Language Model (LLM), to generate a response. \n",
    "\n",
    "We provide the model with 2 things:\n",
    "- our query\n",
    "- the most relevant documents retrieved in the previous step\n",
    "\n",
    "We use a PromptTemplate, which is a set of instructions for our LLM. It's like telling a story to a friend and then asking them a question about that story.\n",
    "\n",
    "In this case, the PromptTemplate instructs the LLM to use the documents (context) to answer the question at the end.\n",
    "\n",
    "<img src=\"images/VectorDatabaseProcess.png\" alt=\"Image Alt Text\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary classes from the langchain library.\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define a prompt template. This is a format for the text input we'll give to our model.\n",
    "# It tells the model how to structure its response and what to do in different situations.\n",
    "template = \"\"\"I will provide you pieces of [Context] to answer the [Question]. \\\n",
    "If you don't know the answer based on [Context] just say that you don't know, don't try to make up an answer. \\\n",
    "[Context]: {context} \\\n",
    "[Question]: {question} \\\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "# If your answer includes any sort of list, return it in bullets. \\\n",
    "# Format your answer to Markdown. \\\n",
    "\n",
    "# Create a PromptTemplate object from our string template.\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Initialize our language model. We're using OpenAI's GPT-3.5-turbo model here.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Create a RetrievalQA object. This uses our language model (llm) and a retriever,\n",
    "# which is our vector database (vectordb). This object will handle asking our model questions\n",
    "# and retrieving relevant documents to help answer them.\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, here is a helpful answer to the question:\n",
      "\n",
      "To start your journey as an aspiring deep learning engineer, it is recommended to follow a top-down approach. This means focusing on applications and practical implementations of deep learning rather than getting overwhelmed by the mathematical equations initially. Look for courses or resources that provide hands-on experience and showcase real-world examples of deep learning in action. \n",
      "\n",
      "Here are some steps you can take:\n",
      "\n",
      "1. Start with a course like the one mentioned in the context, specifically designed for people who are comfortable with statistics, math, Python programming, and classical machine learning. This course will provide a structured introduction to deep learning, starting from pre-deep learning methods and gradually progressing towards more advanced concepts.\n",
      "\n",
      "2. Once you have a basic understanding, explore and experiment with popular deep learning models and frameworks. For computer vision, you can try YOLO-NAS for image classification and ChatGPT or other language models for natural language processing tasks. Interact with these models and try building something cool with them.\n",
      "\n",
      "3. To gain deeper insights, consider resources like Andrew Glassner's deep learning crash course, which provides proper intuition for understanding how deep learning works. Another recommendation is the book \"Deep Learning: A Visual Approach\" by Jon Krohn, which offers a visual perspective on deep learning concepts.\n",
      "\n",
      "4. Join communities and engage with other deep learning enthusiasts. Surrounding yourself with people who have different levels of experience, from beginners to experts, can provide valuable insights and support.\n",
      "\n",
      "5. Finally, work on projects to apply your knowledge and gain practical experience. Platforms like Kaggle offer datasets and competitions where you can participate and solve real-world problems using deep learning techniques.\n",
      "\n",
      "Remember, the key is to start with practical applications, gradually build your understanding, and continuously engage with the deep learning community. Good luck on your journey!\n"
     ]
    }
   ],
   "source": [
    "question = \"I'm an aspiring deep learning engineer. How should I start?\"\n",
    "\n",
    "# Ask our question to the qa_chain, and store the result.\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "# Print out the result\n",
    "print(result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Harpreet Sahota:  00:36:11  Yeah, yeah. Doing it on LinkedIn Learning, it's, it's, it's \\ngonna  be a cool course. So, like it, the audience for this \\ncourse are people who are like me before I got into deep \\nlearning. So, if you're comfortable with statistics, math, \\nPython programming, classical ML, if like, you're good \\nwith all that, and you're like looking at this deep learning \\nthing and wondering like, okay, how, how can I get into \\nthis? Then this is the course that I made for you. I made \\nit for an earlier version of me. And it goes through, like I \\nstart with like a history of computer vision for im age \\nclassification, and I talk about, you know, important \\nconcepts like the things that I felt I needed to understand \\nbefore I got into deep learning. So, I kind of structured it \\nthat way. I start from pre -deep learning methods, just \\nbriefly touch on those .\", metadata={'page': 19, 'source': 'transcripts/PT693-Transcript.pdf'}),\n",
       " Document(page_content=\"know, pick up YOLO -NAS and run, run it on some image, \\nsee the power  of it. Open up, you know, ChatGPT or any \\nof the other language models and start playing with it, \\nlike interacting with it. Start interacting with models, \\ntrying to build something with it, trying to do cool stuff \\nwith it, right?  \\n 01:08:01  You know, open u p, learn some LangChain, and see what \\nyou can build, right? Just see the magic happen, get \\ninspired. Then once you're kind of inspired, right, if you \\nthink it's cool, right, some people probably won't think it's \\ncool, they'll just, you know, be like, okay,  cool, whatever. \\nThat's fine. But if you think it's cool and you're interested, \\nthen dig a little bit deeper. And digging deeper, there's \\nlike a couple of places I recommend one of them: Andrew \\nGlassner has this deep learning crash course. It's like \\nthree and a half hours long. But it gives you just proper \\nintuition for how, how all this works. Very good return on \\ntime investment. So, like he wrote this book, the Deep \\nLearning Illustrated Guide, huge, huge, massive book, \\nright. No math - \\nJon Krohn:  01:08:48  Deep Learning: A Visual Approach.\", metadata={'page': 35, 'source': 'transcripts/PT693-Transcript.pdf'}),\n",
       " Document(page_content=\"to more refined use cases. T hat makes a lot of sense to \\nme. So obviously you're learning a lot about deep learning \\nin particular, and I know that you have a particular \\nphilosophy of learning deep learning that you describe as \\ntop-down. Do you want to describe for our listeners what \\nthat means and why it might also be the way that they \\nshould be learning complex concepts like deep learning?  \\nHarpreet Sahota:  01:07:08  Yeah, yeah. So, I'll, I'll preface this by saying that like, \\nyou know, I've got a master's in mathematics and \\nstatistics.  I was a biostatistician, I was the actuary, I was \\na data scientist. So, there is, this is coming from that \\nperspective, but even with, as somebody who has that \\nbackground, like my approach is to just skip the math. \\nFirst, skip the math, right? Ignore it w hen you're starting \\nout, because looking at equations is gonna demotivate \\nyou, right? So, what I instead implore people to do is just \\nlook for applications of deep learning, right? So, you\", metadata={'page': 34, 'source': 'transcripts/PT693-Transcript.pdf'}),\n",
       " Document(page_content=\"Neural Network Zero to Hero on YouTube. Great, great \\nresource for that. You actually end up building you end \\nup building like a mini version of PyTorch. I think he calls \\nit like minigrad or something like that. But it's amazing. \\nIt's great.  \\n 01:10:47  Once you've done that, then get an understanding of \\nmore foundational architectures. You could, you know, \\nonce my LinkedIn learning course is out, go through that, \\ngo through some of the foundational computer vision \\narchitectures. Yan nic Kilcher has a great YouTube series \\non classical papers. It breaks it down in an easy -to-\\nunderstand manner. And then just join some community, \\nyou know, be around other people who are into the same \\nstuff. You want to be around people who have a broad \\nrange of experience from learners to experts. And then \\nfinally just projects. Just do projects, get on Kaggle, do a\", metadata={'page': 36, 'source': 'transcripts/PT693-Transcript.pdf'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Section\n",
    "\n",
    "### Understanding Retrieval from Vector Databases\n",
    "Vector databases work like a magical library. \n",
    "\n",
    "When you ask a question, the database doesn't read through all the books (or in our case, document splits). Instead, it translates your question into a special language (the embeddings) and then finds the books that speak the same language the closest.\n",
    "\n",
    "These databases use a measure of similarity, such as cosine similarity to find the most related vectors (or embeddings). In our project, `similarity_search()` does exactly this - it finds the most similar vectors (embeddings) to the query vector using cosine similarity.\n",
    "\n",
    "If you think of vectors as arrows in space, cosine similarity measures the cosine of the angle between them. \n",
    "\n",
    "When the vectors are close to each other, pointing in almost the same direction, the cosine of the angle between them is close to 1, meaning they're very similar. On the contrary, if the vectors point in completely different directions, the cosine is close to -1, meaning they're very dissimilar.\n",
    "\n",
    "To capture the essence:\n",
    "- **vectors are close** (pointing in almost the same direction) -> high cosine similarity (close to 1) -> similar meaning\n",
    "- **vectors are far apart** (pointing in almost the opposite direction) -> low cosine similarity (close to -1) -> dissimilar meaning\n",
    "\n",
    "This is how vector databases can quickly find the most related documents to your question!\n",
    "\n",
    "\n",
    "### Context Length in Large Language Models\n",
    "When a language model reads text, it has a limit to how much it can remember at once. \n",
    "\n",
    "This limit is called the \"context length\".\n",
    "\n",
    "Imagine you're reading a very long story but you have a memory limit. If the story exceeds this limit, you start to forget the earlier parts as you read further. \n",
    "\n",
    "The same happens with language models. \n",
    "\n",
    "For the standard GPT-3.5, the context length is 4096 tokens (~3000 words). \n",
    "\n",
    "If a text exceeds this limit, the model can't remember the initial parts while processing the later parts.\n",
    "\n",
    "Context length matters because the quality of the response can significantly depend on the provided context. If important information is outside of the model's context length, it won't be able to reference it in its response.\n",
    "\n",
    "### Vector Databases are the modern solution for the Context Length limits\n",
    "\n",
    "We can't feed our Large Language Models with a 300-page PDF.\n",
    "\n",
    "The Context Length is too short. The model won't \"remember\" most of it.\n",
    "\n",
    "But we can feed our Vector Database with a 300-page PDF!\n",
    "\n",
    "Thanks to similarity search, we can retrieve ONLY the relevant chunks from our PDF.\n",
    "\n",
    "Then, we just take our query together with the chunks without exceeding the context length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
